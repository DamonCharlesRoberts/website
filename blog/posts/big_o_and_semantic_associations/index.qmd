---
title: "The brain as an extremely efficient and organic computer: Big O Notation, Algorithms, and Associative Networks"
author: "Damon C. Roberts"
date: 10/11/2023
categories:
    - Big O Notation
    - Algorithms
    - Cognition
draft: true
format:
  html:
    code-tools: false
bibliography: "../../assets/algorithms.bib"
execute:
    message: false
    warning: false
    echo: false
---

<!--

Fix tortured analogy. Switch it to something like your friend asking you to grab something out of a cabinet

-->

As someone who has a background in scientific research on political cognition and who applies statistical tools from computer science to social scientific theories (computational social science), I am deeply fascinated by the ways in which our computers and major computer science theories are inspired by theories of human cognition -- even if, and perhaps especially, when they are developed in parallel. Because of my interest in both areas, I thought it might be fun to write a blog post about my limited, but growing, knowledge of these two things.

# The brain as an organic computer

In a likely familiar view of the human brain, we have some type of experience. With that experience, we encode all of the information that we collected from it and store it into our memory [@kahana_laws_2022]. The degree to which that information is retained and is accessible throughout our lives depends on a dizzying number of factors [see @kahana_laws_2022]. However, if we dig in deeper on questions of how this encoding happens, how we can misremember things, and how we can connect various memories with each other, then we get into a fascinating territory of research on cognition.

An extremely influential conceptualization of how the human brain works is that it works like a network. What this means is that memories are not just discrete or isolated pieces of information stored in particular neurons. While we can get really complicated with the science of how neurons play into this story, the idea is that memories are actually just connections to interrelated things throughout the brain. Let's think of this conceptualization from an abstract level. Say that we have a set of memories that look like [@fig-1] below.

```{mermaid}
%%| label: fig-1
%%| fig-cap: Abstract view of the brain as an associative network

flowchart LR;
A(("Red"))
B(("Fruit"))
C(("Vehicles"))
D(("Clothes"))
A -.-> E(("Cherries"))
B --> E
A -.-> F(("Fire Truck"))
C --> F
A -.-> G(("Red Sweater"))
D --> G
```

Say that we are walking down a sidewalk with a friend. In the middle of you finishing your sentence, they try to grab your attention by yelling, "Look at that red thing over there!" There are definitely a lot of things that can be red, right? So, while red gives you one dimension by which you can figure out what they are trying to point your attention to, it still doesn't give you complete information about what you are looking for. To figure out what exactly they are trying to point your attention to, you might be trying to narrow it down more by considering other information. If you are walking along a road, you may look for a red vehicle and figure out it is a fire truck. Instead, you may in a grocery store or a department store and can look for either fruit or clothing that is red. With you shifting your attention based on these other situational cues, you may be able to figure out that they are pointing out red cherries or a red sweater. The context by which you found yourself in, it helped you sort through what alternative features of the mysterious red object were good fits.

What does this tell us about how the human brain works? Well, for one, we can use our memories to evaluate new information that is coming in. We can remember that cherries are red, fire trucks (tend to be) red, and that we can sport red sweaters. The other idea that it tells us is that memories are stored in a multi-dimensional space [@fazio_attitudes_2007;@kahana_laws_2022]. We do not just recall that sweaters are red, but we can know that sweaters come in a bunch of different colors with one of them being red. Likewise, we do not remember that red is synonymous with a fire truck but that red can describe a whole lot of different things.

It isn't just brains that work this way. While cognitive scientists crank away at understanding how humans store and quickly access information, computer scientists have also spent a significant amount of effort thinking through efficient ways to store and quickly access information. In some early efforts to think about how we can store semantic information in a computer's memory, @quillian_word_1967 modeled this information as being established as a network. Like in what I depicted in @fig-1, if our computer is tasked with finding a red object, it might need some additional information to help it. This additional information is extremely varied but can quickly increase the processing speed the better that additional information is. Like the human brain, the stronger the connection between the nodes the easier it is to access the object. For example, not all fruits are red, but many more are red than sweaters; therefore, the strength between the nodes red, fruits, and cherries are stronger than red, clothes, and sweaters.

Okay, Damon what is the point of this?

# The brain's efficient algorithms

If we asked our computer to check whether some object is red or not, one thing we could do would be to have it write a for-loop as you see in the pseudo-code below:

```
for i in item
    if object is red
        print "Is this the item you are talking about?"
    
else
    print "I don't see anything red here."
```

With this program, our computer would be going through each possible thing and will only stop once it finds something that is red. Like the brain, this forgets that it can still be inaccurate because it could identify a red item that is not what we were talking about. Accuracy concerns aside, this process is extremely **slow**. For-loops are a classic example of an extremely process characterized with the Big O notation of $O(n)$. Meaning, on the upper-end of how many steps our computer had to take to find a red item, it could take the computer theoretically as many steps as we have items that we are asking our computer to search through. In the best case scenario, we could use Omega notation and it would be $\omega(1)$. In this ideal case, the computer could identify the red item in the very first loop through the list of items you are searching through. However, it is much more likely to take longer to do.

But like we did in our own heads, we wanted to sort this information in some sort of way to make the discovery of what your friend was pointing out quicker. So, what we can do is we can use various context clues. What we can do, is we can sort through items that make the most sense to make our process easier and start from there. One common and powerful algorithm is called a Binary Search Tree. 